{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11244  100 11244    0     0  88068      0 --:--:-- --:--:-- --:--:-- 87843\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2844  100  2844    0     0  42615      0 --:--:-- --:--:-- --:--:-- 43090\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!curl --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
        "!curl --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    return sum(a*b for a, b in zip(array1, array2))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    #TODO: return the output of the model \n",
        "    return sigmoid(dot_product(weight, instance[:-1]))\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    #TODO: return the prediction of the model\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: name this step\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: name these steps\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: name these steps\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "50YvUza-BYQF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "The reason the first code snippet is used is because the first snippet calculates the continuous output of the perceptron model using the sigmoid function, which assists in computing the gradient and subsequent updates of weights. By using the sigmoid function, a smooth gradient is used to adjust the weights in a far more precise manner based on how far off the prediction is. The 'predict' function only provides a binary 1 or 0 depending on whether the sigmoid function is above or below 0.5. By using a binary output, gradient information, which is necessary or weight updates during training.The gradient allows for nuanced adjustments to the weights that the binary model would not be able to provide. The prediction would be either 0 or 1, not a continuous value that represents the confidence of the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G-VKJOUu2BTp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 20, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0%\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0%\n",
            "#tr: 300, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 300, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 300, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 300, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0%\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 400, epochs: 5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 400, epochs: 10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 400, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0%\n",
            "#tr: 400, epochs: 50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0%\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0%\n",
            "#tr: 20, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0%\n",
            "#tr: 200, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 200, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0%\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 300, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 300, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 300, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0%\n",
            "#tr: 300, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0%\n",
            "#tr: 400, epochs: 5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 400, epochs: 10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0%\n",
            "#tr: 400, epochs: 20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0%\n",
            "#tr: 400, epochs: 50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0%\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0%\n",
            "#tr: 20, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0%\n",
            "#tr: 40, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 40, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0%\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0%\n",
            "#tr: 100, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0%\n",
            "#tr: 100, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0%\n",
            "#tr: 100, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0%\n",
            "#tr: 100, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0%\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0%\n",
            "#tr: 200, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0%\n",
            "#tr: 200, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0%\n",
            "#tr: 200, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 200, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0%\n",
            "#tr: 300, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0%\n",
            "#tr: 300, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 300, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0%\n",
            "#tr: 300, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0%\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0%\n",
            "#tr: 400, epochs: 5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0%\n",
            "#tr: 400, epochs: 10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0%\n",
            "#tr: 400, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0%\n",
            "#tr: 400, epochs: 50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0%\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0%\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "\n",
        "# Define the arrays for the hyperparameters\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]         # learning rate\n",
        "\n",
        "# Iterate over each combination of hyperparameters\n",
        "for lr in lr_array:\n",
        "    for percent in tr_percent:\n",
        "        size = round(len(instances_tr) * percent / 100)\n",
        "        training_subset = instances_tr[:size]\n",
        "        \n",
        "        for epochs in num_epochs:\n",
        "            # Train the perceptron with the current subset, learning rate, and epochs\n",
        "            weights = train_perceptron(training_subset, lr, epochs)\n",
        "            \n",
        "            # Calculate the accuracy on the test dataset\n",
        "            accuracy = get_accuracy(weights, instances_te)\n",
        "            \n",
        "            # Print the results for this combination\n",
        "            print(f\"#tr: {len(training_subset)}, epochs: {epochs}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "A. The data shows that using the entire training dataset with a higher learning rate (0.05) and more epochs leads to the highest accuracy of 80%. For this problem, it is clear that having more data available for training helps the model to learn and generalize more effectively when using new data. The improvement in accuracy is not linear with the amount of training data, and there are diminishing returns as the dataset size increases, which indicates that while using the entire dataset can be beneficial, it is not always necessary to achieve high accuracy.\n",
        "\n",
        "B. When comparing the two runs, the difference in accuracy can be attributed to the learning rate. A higher learning rate (0.05) with fewer data points (100 instances) resulted in better accuracy than a lower learning rate (0.005) with more data (200 instances). A suitable learning rate can help the model to converge more quickly to a good solution, even with less data, which depicts the importance of the learning rate in the training process. However, it also brings to attention the trade-off and balance required between learning rate and dataset size, meaning that a higher learning rate may not always be better, especially if it causes the model to overshoot the minimum of the loss function.\n",
        "\n",
        "C. The highest accuracy observed is 80.0% with specific hyperparameters as presented . Achieving higher accuracy (above 80.0%) could be possible by further tuning hyperparameters, introducing regularization to prevent overfitting, or using more sophisticated models like deep neural networks that can capture complex patterns in the data more effectively. However, with the simple perceptron model in this problem, there might be a limit to how much accuracy can be improved without overfitting the training data.\n",
        "\n",
        "D. Increasing the number of epochs in model training, with all other hyperparameters held constant, does not guarantee improved performance indefinitely because it can lead to overfitting. When overfitting occurs, the model learns the noise in the training data rather than the underlying pattern. This overfitting results in poorer generalization to new data. The optimal number of epochs depends on various factors, including the complexity of the model along with the learning rate and the size and diversity of training data. Monitoring the model's performance on a validation set to determine the best point to stop training is essential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW2_The_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
